{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "- Due: 11:59pm, April 26, 2019\n",
    "\n",
    "In this project, you will work on sentiment classification with a logistic regression classifier in Python 3.  Using a large movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/), you will classify movie reviews into two categories, POSITIVE or NEGATIVE. \n",
    "\n",
    "You are provided with a training set (TRAIN), a development set (DEV), and a test set (TEST). Your classifier will be trained on TRAIN, evaluated and tuned on DEV, and tested on TEST. \n",
    "\n",
    "Using the PyTorch library, you will build the logistic regression classifier with bag of words features.  Some code has been provide  to help get you started.\n",
    "\n",
    "You need to fill in the missing code, run all cells, and submit this notebook along with a PDF with a writeup on your model tuning results and  solutions to the other problems in Homework 1.\n",
    "\n",
    "Credits: This assignment and notebook was originally created by Zewei Chu (zeweichu@uchicago.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# Feel free to define your own word_tokenizer instead of this naive \n",
    "# implementation. You may also use word_tokenize from nltk library \n",
    "# (from nltk import word_tokenize), which works better but slower. \n",
    "def word_tokenize(s):\n",
    "    return s.split()\n",
    "\n",
    "# set the random seeds so the experiments can be replicated exactly\n",
    "seed = 30255\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    data = []\n",
    "    with open(data_file,'r') as fin:\n",
    "        for line in fin:\n",
    "            label, content = line.split(\",\", 1)\n",
    "            data.append((content.lower(), label))\n",
    "    return data\n",
    "data_dir = \"large_movie_review_dataset\"\n",
    "train_data = load_data(os.path.join(data_dir, \"train.txt\"))\n",
    "dev_data = load_data(os.path.join(data_dir, \"dev.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of TRAIN data 25000\n",
      "number of DEV data 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of TRAIN data\", len(train_data))\n",
    "print(\"number of DEV data\", len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a generic model class as below. The model has 2 functions, train and classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "class Model:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.vocab = Counter([word for content, label in data \n",
    "                              for word in word_tokenize(content)]\n",
    "                            ).most_common(VOCAB_SIZE-1)\n",
    "        # word to index mapping\n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.vocab)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.word_to_idx[\"UNK\"] = 0 \n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.idx_to_label = [POS_LABEL, NEG_LABEL]\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError \n",
    "\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        Classify the documents with the model\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(train_data)\n",
    "vocab = Counter([word for content, label in train_data \n",
    "                              for word in word_tokenize(content)]\n",
    "                            ).most_common(VOCAB_SIZE-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Bag of Words\n",
    "\n",
    "(65 points)\n",
    "\n",
    "You will implement logistic regression with bag of words features. The code template is written with PyTorch. Reading the first two sections of the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html) will give you enough knowledge to code a logistic regression model with PyTorch. \n",
    "\n",
    "(When used for deep learning PyTorch code is usually run on GPUs (via the CUDA system).  In this homework, however, we'll use regular CPUs.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    PyTorch provides a common dataset interface. \n",
    "    See https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    The dataset encodes documents into indices. \n",
    "    With the PyTorch dataloader, you can easily get batched data for \n",
    "    training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, word_to_idx, data):\n",
    "        \n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = np.zeros(self.vocab_size)\n",
    "        \n",
    "        item = torch.from_numpy(item)\n",
    "        # in training or tuning, we use both the document (review)\n",
    "        # and its corresponding label\n",
    "        if len(self.data[idx]) == 2: \n",
    "            for word in word_tokenize(self.data[idx][0]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            label = self.label_to_idx[self.data[idx][1]]\n",
    "            return item, label\n",
    "        else: # in testing, we only use the document without label\n",
    "            for word in word_tokenize(self.data[idx]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "class BoWLRClassifier(nn.Module, Model):\n",
    "    '''\n",
    "    Define your logistic regression model with bag of words features.\n",
    "    '''\n",
    "    def __init__(self, train_data):\n",
    "        nn.Module.__init__(self)\n",
    "        Model.__init__(self, train_data)\n",
    "     \n",
    "        '''\n",
    "        In this model initialization phase, write code to do the \n",
    "        following: \n",
    "        1. Define a linear layer to transform bag of words features \n",
    "           into 2 classes. \n",
    "        2. Define the loss function; use cross entropy loss (see\n",
    "            https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss)\n",
    "        3. Define an optimizer for the model; choose the Adam optimizer,\n",
    "           which uses a version of the stochastic gradient descent \n",
    "           algorithm. (See https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.Adam)\n",
    "        '''\n",
    "        \n",
    "        self.linear = nn.Linear(VOCAB_SIZE, 2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "        self.train_data = TextClassificationDataset(self.word_to_idx, train_data)\n",
    "        \n",
    "    def forward(self, bow):\n",
    "        '''\n",
    "        Run the linear layer in the model for a single bag of words vector. \n",
    "        '''\n",
    "        # WRITE YOUR CODE HERE\n",
    "        # (You might be wondering why we don't explicitly have a\n",
    "        # softmax component in our model. It is included in something\n",
    "        # defined earlier. In what?)\n",
    "        # Note: the softmax component is included in the loss function\n",
    "        \n",
    "        bow = bow.float()\n",
    "        return self.linear(bow)\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        '''\n",
    "        Train the model for one epoch with the training data\n",
    "        When training a model, you repeat the following procedure:\n",
    "        1. Get one batch of features and labels\n",
    "        2. Make a forward pass with the features to get predictions\n",
    "        3. Calculate the loss with the predictions and target labels\n",
    "        4. Run a backward pass from the loss function to get the gradients\n",
    "        5. Apply the optimizer step to update the model paramters\n",
    "        \n",
    "        For (1) you will have to understand how the PyTorch dataloader\n",
    "        functions.\n",
    "        '''\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance (from tutorial)\n",
    "        #model.zero_grad()\n",
    "        \n",
    "        dataloader = DataLoader(self.train_data, batch_size=8,\n",
    "                        shuffle=True, num_workers=4)\n",
    "        \n",
    "        \n",
    "        for bows, targets in dataloader:\n",
    "            prediction = self.forward(bows)\n",
    "            loss = self.loss(prediction, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def classify(self, doc):\n",
    "        '''\n",
    "        This function classifies a single document into its categories. \n",
    "        the input is a document that has been processed into a bag of words.\n",
    "        '''\n",
    "                \n",
    "        return self.forward(doc)\n",
    "        \n",
    "    def evaluate_classifier_accuracy(self, data):\n",
    "        '''\n",
    "        This function evaluates the data with the current model. \n",
    "        data contains both documents and labels. \n",
    "        It calls classify() to make predictions, \n",
    "        and compares with the correct labels to return \n",
    "        the model accuracy on \"data\". \n",
    "        '''\n",
    "        \n",
    "        denom = len(data)\n",
    "        correct = 0\n",
    "        \n",
    "        # need absolute value?\n",
    "        for bow, target in data:\n",
    "            prediction = self.classify(bow.float())\n",
    "            \n",
    "            if prediction[0] < prediction[1]:\n",
    "                classification = 1\n",
    "            else:\n",
    "                classification = 0\n",
    "            if classification == target:\n",
    "                correct += 1\n",
    "        \n",
    "        return correct/denom\n",
    "                \n",
    "    \n",
    "    def train_model(self, train_data, dev_data):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\"  \n",
    "        dev = TextClassificationDataset(self.word_to_idx, dev_data)\n",
    "        \n",
    "        for epoch in range(5):\n",
    "            self.train_epoch()\n",
    "            accuracy = self.evaluate_classifier_accuracy(dev)\n",
    "            print(\"The accuracy for epoch {} is {}\".format(epoch, accuracy))\n",
    "\n",
    "# zero grad? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = BoWLRClassifier(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for epoch 0 is 0.8072\n",
      "The accuracy for epoch 1 is 0.823\n",
      "The accuracy for epoch 2 is 0.8176\n",
      "The accuracy for epoch 3 is 0.8306\n",
      "The accuracy for epoch 4 is 0.8054\n"
     ]
    }
   ],
   "source": [
    "lr_model.train_model(train_data, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = []\n",
    "for param in lr_model.parameters():\n",
    "    param_list.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.4454,  -0.4569,  -0.7188,  ..., -13.5333,  14.6355, -14.9937],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = param_list[0][]\n",
    "val, idx = pos.max(0)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seagal\n",
      "awful.\n",
      "lacks\n",
      "uninteresting\n",
      "mst3k\n",
      "forgettable\n",
      "struggling\n",
      "disappointing\n",
      "unconvincing\n",
      "pathetic.\n"
     ]
    }
   ],
   "source": [
    "neg = param_list[0][1]\n",
    "idx_t = torch.topk(pos, k=10, dim=0)[1]\n",
    "for idx in idx_t:\n",
    "    print(lr_model.idx_to_word[idx.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criticism\n",
      "r\n",
      "8/10\n",
      "amazing.\n",
      "perfect.\n",
      "tight\n",
      "excellent.\n",
      "faced\n",
      "contrast\n",
      "rural\n"
     ]
    }
   ],
   "source": [
    "pos = param_list[0][0]\n",
    "idx_t = torch.topk(pos, k=10, dim=0)[1]\n",
    "for idx in idx_t:\n",
    "    print(lr_model.idx_to_word[idx.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "topk() got an unexpected keyword argument 'smallest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-1c33a76f4476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmallest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: topk() got an unexpected keyword argument 'smallest'"
     ]
    }
   ],
   "source": [
    "pos = param_list[0][0]\n",
    "idx_t = torch.topk(pos, k=10, dim=0, smallest=True)[1]\n",
    "for idx in idx_t:\n",
    "    print(lr_model.idx_to_word[idx.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_model.train_model(train_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "(25 points)\n",
    "\n",
    "Now tune your model, by experimenting with\n",
    "\n",
    "- another optimizer\n",
    "- changing the learning rate\n",
    "- changing the number of epochs to train\n",
    "- adding regularization into your optimzer.\n",
    "\n",
    "Finally evaluate your tuned model on the TEST set.\n",
    "\n",
    "Report your results in a writeup, and submit that as a\n",
    "separate PDF file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store best model, accuracy, epoch\n",
    "\n",
    "optimizers = []\n",
    "best_model = copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis\n",
    "\n",
    "(10 points)\n",
    "\n",
    "Write code for each of the following, and include an analysis of the results in your writup.\n",
    "\n",
    "\n",
    "- Identify the top 10 features with the maximum weights for POSITIVE category. \n",
    "\n",
    "- Identify the top 10 features with the maximum negative weights for POSITIVE category. \n",
    "\n",
    "- Identify the top 10 features with the maximum positive weights for NEGATIVE category. \n",
    "\n",
    "- Identify the top 10 features with the maximum negative weights for NEGATIVE category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE FOR FEATURE ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
